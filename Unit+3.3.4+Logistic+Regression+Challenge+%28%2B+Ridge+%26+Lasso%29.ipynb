{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 3\\\\breast_cancer_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Attribute Information:\n",
    "#1) ID number\n",
    "#2) Diagnosis (M = malignant, B = benign)\n",
    "#-3-32.Ten real-valued features are computed for each cell nucleus:\n",
    "#a) radius (mean of distances from center to points on the perimeter)\n",
    "#b) texture (standard deviation of gray-scale values)\n",
    "#c) perimeter\n",
    "#d) area\n",
    "#e) smoothness (local variation in radius lengths)\n",
    "#f) compactness (perimeter^2 / area - 1.0)\n",
    "#g). concavity (severity of concave portions of the contour)\n",
    "#h). concave points (number of concave portions of the contour)\n",
    "#i). symmetry\n",
    "#j). fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This dataset has 10 'unique' features, and three versions of these features (mean, SE, and 'worst', meaning the avg of the \n",
    "#3 largest... see dataset description here: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\n",
    "\n",
    "#The point is, the data should show some multicolinearity..meaning that our ridge regression should come in handy here, \n",
    "#and that we could see some higher than usual coefficients in the regular logistic regression model\n",
    "\n",
    "#Convert diagnosis to binary from M (malignant) and B (benign)\n",
    "\n",
    "df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df['Unnamed: 32'].unique())\n",
    "#All values in this column are NAN, so drop it, and also drop the ID column\n",
    "df.drop('Unnamed: 32', axis = 1, inplace = True)\n",
    "df.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean           ...             radius_worst  texture_worst  \\\n",
       "0         0.2419           ...                    25.38          17.33   \n",
       "1         0.1812           ...                    24.99          23.41   \n",
       "2         0.2069           ...                    23.57          25.53   \n",
       "3         0.2597           ...                    14.91          26.50   \n",
       "4         0.1809           ...                    22.54          16.67   \n",
       "\n",
       "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0           184.60      2019.0            0.1622             0.6656   \n",
       "1           158.80      1956.0            0.1238             0.1866   \n",
       "2           152.50      1709.0            0.1444             0.4245   \n",
       "3            98.87       567.7            0.2098             0.8663   \n",
       "4           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0           0.7119                0.2654          0.4601   \n",
       "1           0.2416                0.1860          0.2750   \n",
       "2           0.4504                0.2430          0.3613   \n",
       "3           0.6869                0.2575          0.6638   \n",
       "4           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Overall Percentage Accuracy\n",
      "0.966608084359\n",
      "Accuracy: Fold 1\n",
      "0.912280701754\n",
      "Accuracy: Fold 2\n",
      "0.956140350877\n",
      "Accuracy: Fold 3\n",
      "0.973684210526\n",
      "Accuracy: Fold 4\n",
      "0.964912280702\n",
      "Accuracy: Fold 5\n",
      "0.946902654867\n",
      "[ -2.45670877e+00   6.33033290e-02  -2.93359288e-02   6.27779905e-03\n",
      "   1.57600177e-01   4.96699759e-01   6.96998896e-01   3.53491799e-01\n",
      "   1.98239804e-01   3.74561653e-02  -3.23224050e-02  -1.57427141e+00\n",
      "  -3.48563541e-01   1.36506864e-01   1.65129875e-02   4.12591558e-02\n",
      "   8.22908235e-02   4.28349255e-02   4.85094635e-02  -2.06870102e-03\n",
      "  -1.91551463e+00   2.91216265e-01   3.00362342e-01   2.58773616e-02\n",
      "   3.04943717e-01   1.59087259e+00   2.00579979e+00   7.14479945e-01\n",
      "   6.87200637e-01   1.67541447e-01  -4.40542725e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#REGULAR LOGISTIC REGRESSION\n",
    "\n",
    "predictors = df.ix[:, 1:]\n",
    "targets = df['diagnosis']\n",
    "\n",
    "#Set the C value very high such that there is essentially no regularization taking place (C set to very high value)\n",
    "lr = LogisticRegression(C = 1e9)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(predictors, targets)\n",
    "\n",
    "print('\\n Overall Percentage Accuracy')\n",
    "print(lr.score(predictors, targets))\n",
    "\n",
    "X = predictors\n",
    "y = targets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_no = 1\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #Use model defined above\n",
    "    predicted = lr.fit(X_train, y_train).predict(X_test)\n",
    "    actual = y_test\n",
    "    print('Accuracy: Fold', test_no)\n",
    "    print(lr.score(X_test, y_test))\n",
    "    test_no = test_no + 1\n",
    "\n",
    "#Store the parameter estimates.\n",
    "origparams = np.append(lr.coef_, lr.intercept_)\n",
    "print(origparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In your report, evaluate all three models and decide on your best. Be clear about the decisions you made that led \n",
    "#to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you \n",
    "#think that particular model is the best of the three. Also reflect on the strengths and limitations of regression \n",
    "#as a modeling approach. Were there things you couldn't do but you wish you could have done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Overall Percentage Accuracy\n",
      "0.966608084359\n",
      "Accuracy: Fold 1\n",
      "0.964912280702\n",
      "Accuracy: Fold 2\n",
      "0.947368421053\n",
      "Accuracy: Fold 3\n",
      "0.973684210526\n",
      "Accuracy: Fold 4\n",
      "0.973684210526\n",
      "Accuracy: Fold 5\n",
      "0.946902654867\n",
      "[ -2.29151998e+00   5.65773698e-02  -4.34668809e-02   5.54213220e-03\n",
      "   1.35592165e-01   4.59717138e-01   6.35770653e-01   3.12964237e-01\n",
      "   1.70935144e-01   3.42258506e-02  -4.39905926e-02  -1.40248453e+00\n",
      "  -4.08637731e-01   1.35792958e-01   1.38725507e-02   5.10724766e-02\n",
      "   8.62945215e-02   3.79099124e-02   4.38306885e-02   4.72105356e-04\n",
      "  -1.90722494e+00   2.77798156e-01   3.02395521e-01   2.54423956e-02\n",
      "   2.60972635e-01   1.47614934e+00   1.83398327e+00   6.29868590e-01\n",
      "   5.97596184e-01   1.54065529e-01  -4.11262277e-01]\n"
     ]
    }
   ],
   "source": [
    "#Now try ridge regression to see if adding a regularization penalty can help improve accuracy\n",
    "\n",
    "#RIDGE REGRESSION (L2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Set the C value lower such that the regularization does have an impact\n",
    "lr = LogisticRegressionCV(penalty='l2', solver = 'liblinear', cv = 5)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(predictors, targets)\n",
    "\n",
    "print('\\n Overall Percentage Accuracy')\n",
    "print(lr.score(predictors, targets))\n",
    "\n",
    "X = predictors\n",
    "y = targets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_no = 1\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #Use model defined above\n",
    "    predicted = lr.fit(X_train, y_train).predict(X_test)\n",
    "    actual = y_test\n",
    "    print('Accuracy: Fold', test_no)\n",
    "    print(lr.score(X_test, y_test))\n",
    "    test_no = test_no + 1\n",
    "\n",
    "#Store the parameter estimates.\n",
    "cvparams = np.append(lr.coef_, lr.intercept_)\n",
    "print(cvparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Overall Percentage Accuracy\n",
      "0.992970123023\n",
      "Accuracy: Fold 1\n",
      "0.921052631579\n",
      "Accuracy: Fold 2\n",
      "0.938596491228\n",
      "Accuracy: Fold 3\n",
      "0.964912280702\n",
      "Accuracy: Fold 4\n",
      "0.982456140351\n",
      "Accuracy: Fold 5\n",
      "0.973451327434\n",
      "[ -2.17839853e+00   2.79088872e-01  -2.82118068e-01   2.51822444e-02\n",
      "   0.00000000e+00  -1.92639781e+01   7.72919698e+00   6.18783271e+01\n",
      "  -1.09138801e+01   0.00000000e+00   2.52539090e+00  -1.25218807e+00\n",
      "  -2.45863940e-01   1.78981848e-01   0.00000000e+00  -2.65447895e+01\n",
      "  -3.71976706e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -9.02909783e-02   2.47755562e-01   7.56253983e-02   1.94544392e-02\n",
      "   4.93670131e+01  -3.78383229e+00   1.00519199e+01   4.79731262e+01\n",
      "   7.24526574e+00   0.00000000e+00  -1.12551022e+01]\n"
     ]
    }
   ],
   "source": [
    "#LASSO REGRESSION (L1)\n",
    "\n",
    "#LOGISTICREGRESSIONCV is supposed to find the optimal C value\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Set the C value lower such that the regularization does have an impact\n",
    "lr = LogisticRegressionCV(penalty='l1', solver = 'liblinear', cv = 5)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(predictors, targets)\n",
    "\n",
    "print('\\n Overall Percentage Accuracy')\n",
    "print(lr.score(predictors, targets))\n",
    "\n",
    "X = predictors\n",
    "y = targets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_no = 1\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #Use model defined above\n",
    "    predicted = lr.fit(X_train, y_train).predict(X_test)\n",
    "    actual = y_test\n",
    "    print('Accuracy: Fold', test_no)\n",
    "    print(lr.score(X_test, y_test))\n",
    "    test_no = test_no + 1\n",
    "\n",
    "#Store the parameter estimates.\n",
    "cvparams = np.append(lr.coef_, lr.intercept_)\n",
    "print(cvparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I chose to use the LogisiticRegressionCV function because it has a built-in solver that finds the optimal C-value\n",
    "#to use for the regularization parameter \n",
    "\n",
    "#Overall, I would opt to use the ridge regresssion (L2) logisticregressioncv function because although the overall model accuracy \n",
    "#was not the highest (96.6% vs 98.6% for LASSO), it performed more consistently on the cross-validation tests with accuracy\n",
    "#ranging from ~95%-97% while the LASSO had less consistent perforamance (accuracy ranged from 92%-98%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Could also fit the data to a random forest to determine which are the most important features, and then re-run the models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
