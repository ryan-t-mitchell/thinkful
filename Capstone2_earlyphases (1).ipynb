{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "#from tweepy.parsers import JSONParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_token = ''\n",
    "consumer_secret = ''\n",
    "\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_token, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida residents want to know why Trump isnâ€™t opening his resorts to hurricane victims https://t.co/H3VGXu05dS Greâ€¦ https://t.co/SvTzB02b79\n",
      "RT @TheRynheart: Trump tells cabinet Hurricane Irma means wealthy need their tax cuts faster. ðŸ˜®\n",
      "@realDonaldTrump are you kidding me? \n",
      "httpsâ€¦\n",
      "RT @Unpersuaded112: The rich get richer and donald ( @realDonaldTrump ) scews the rest of us #trump #trumpTaxPlan https://t.co/PMKQWG0Tsa\n",
      "@ajulysantos Tem que rir mesmo. E o Catrina que acabou com New Orleans. EstÃ¡ fixaÃ§Ã£o do Trump e doentia.\n",
      "RT @SteveBannen: Trump's response to Hillary Clinton's newly-released book:\n",
      "\n",
      "\"You mad, bro?\"\n",
      "RT @todoalnatural: A ver si despuÃ©s de esto, Donald Trump sigue diciendo que el 'cambio climÃ¡tico' es puro cuento. https://t.co/jEia7Fz6LA\n",
      "RT @tedlieu: Trump Jr. testimony confirms the initial public statement about the Russia meeting by @DonaldJTrumpJr &amp; @realDonaldTrump was aâ€¦\n",
      "Trump to go to Florida 'very soon': https://t.co/PWn8baS6N8 via @YouTube\n",
      "RT @ProgressOutlook: Robert Mercer, who gave millions to Trump's campaign, also bankrolled neo-Nazi and pedophile Milo Yiannopolous.\n",
      "RT @samstein: UPDATE: Trump was just asked if he thinks climate change is related to these storms. he didnâ€™t answer\n",
      "\n",
      "https://t.co/lJVuUDZZJâ€¦\n",
      "RT @dcexaminer: Republicans rip Sessions for not prosecuting Lois Lerner in IRS scandal: \"A miscarriage of justice\" https://t.co/F0u6wFGPU5â€¦\n",
      "RT @EricBoehlert: as I've noted, every news org in America got duped by Trump--it was so obvious he words were hollow when he \"announced\" nâ€¦\n",
      "RT @LiberalResist: The Trump Impeachment - Trump Admits President Obama Was Right About \"Rising Seas\" https://t.co/dNo2sJEJDf https://t.co/â€¦\n",
      "RT @oliviagobrien: it's so simple: if u agree w trump and wanna get rid of DACA, you are lacking basic human compassion. #defendDACA\n",
      "RT @Impeach_D_Trump: RT If you think the Bushes, Jimmy Carter, Bill Clinton &amp; Barack Obama should make a joint statement calling on Racistâ€¦\n"
     ]
    }
   ],
   "source": [
    "trump = api.search('Trump')\n",
    "for tweet in trump:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'lang'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_2 = api.user_timeline(screen_name = 'realDonaldTrump', count = 200)\n",
    "\n",
    "#What kind of stuff can we do with this object? (tweepy.models.ResultSet)\n",
    "\n",
    "trump_2[0]._json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next step: clean tweets by removing RTs, @_____, and URLs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TWEETS_PER_CALL = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tweets(screen_nm, tweets):\n",
    "    with open(f'C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 4\\\\Capstone\\\\{screen_nm}.json', 'w') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "\n",
    "def get_tweets(screen_nm, desired_ct):\n",
    "    \n",
    "    #desired_ct is the number of tweets the user wants to include\n",
    "    tweet_list = []\n",
    "\n",
    "    #curr_max_id = math.inf\n",
    "    #grab the current maximum tweet id for provided screen name, which will be the id of the first tweet stored in the object\n",
    "    curr_max_id = api.user_timeline(screen_name = screen_nm)[0].id \n",
    "    \n",
    "    #may have to do more than 1 call to the API (if user wants > TWEETS_PER_CALL tweets)\n",
    "    loops = math.ceil(desired_ct / TWEETS_PER_CALL)\n",
    "    remaining = desired_ct\n",
    "        \n",
    "    for iter in range(loops):\n",
    "        tweets = api.user_timeline(screen_name = screen_nm, count = min(remaining, TWEETS_PER_CALL), max_id = curr_max_id, include_rts=False)\n",
    "        \n",
    "        #save these tweets in a json file for later, in case kernel crashes or anything\n",
    "        save_tweets(screen_nm, tweets)\n",
    "        \n",
    "        #because we are excluding RTs, the len(tweets) will almost always be < desired_ct\n",
    "        for i in range(min(remaining, TWEETS_PER_CALL, len(tweets))):\n",
    "            tweet_list.append([tweets[i].text, screen_nm])\n",
    "                \n",
    "        remaining = remaining - min(remaining, TWEETS_PER_CALL)\n",
    "        \n",
    "        #re-set the maximum id for every TWEETS_PER_CALL tweets, so that we don't include duplicate tweets\n",
    "        curr_max_id = tweets[-1].id \n",
    "\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = get_tweets('realDonaldTrump', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton = get_tweets('HillaryClinton', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lists will not actually be 2000 tweets long, due to excluded RT's. Let's use the first 1500 of the final sets\n",
    "\n",
    "df = pd.DataFrame(trump[0:1500]+clinton[0:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Tweets', 'Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The U.S. Coast Guard, FEMA and all Federal and...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heed the advice of @FLGovScott!\\n\\n\"If you're ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a storm of enormous destructive power,...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLORIDA- \\nVisit https://t.co/pdBaD9t8SK to fi...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Churches in Texas should be entitled to reimbu...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets           Author\n",
       "0  The U.S. Coast Guard, FEMA and all Federal and...  realDonaldTrump\n",
       "1  Heed the advice of @FLGovScott!\\n\\n\"If you're ...  realDonaldTrump\n",
       "2  This is a storm of enormous destructive power,...  realDonaldTrump\n",
       "3  FLORIDA- \\nVisit https://t.co/pdBaD9t8SK to fi...  realDonaldTrump\n",
       "4  Churches in Texas should be entitled to reimbu...  realDonaldTrump"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_doc = nlp(str(df['Tweets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(doc):\n",
    "    allwords = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.like_url and \n",
    "                not token.like_num and not token.is_space]\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump',\n",
       " 'hurricane',\n",
       " 'be',\n",
       " 'want',\n",
       " 'president',\n",
       " 'donald',\n",
       " 'â€™s',\n",
       " 'republicans',\n",
       " 'close',\n",
       " 'campaign',\n",
       " 'family',\n",
       " 'hillary',\n",
       " 'u.s.',\n",
       " 'coast',\n",
       " 'guard',\n",
       " 'tax',\n",
       " 'hear',\n",
       " 'irma',\n",
       " 'speak',\n",
       " 'world',\n",
       " 'daca',\n",
       " 'thank',\n",
       " 'great',\n",
       " 'north',\n",
       " 'dakota',\n",
       " 'w/',\n",
       " 'look',\n",
       " 'go',\n",
       " \"'s\",\n",
       " 't',\n",
       " 'stake',\n",
       " 'refuse',\n",
       " 'run',\n",
       " 'america',\n",
       " 'leave',\n",
       " 'wrong',\n",
       " 'way',\n",
       " 'â€™re',\n",
       " 'not',\n",
       " 'election',\n",
       " 'people',\n",
       " 'fema',\n",
       " 'federal',\n",
       " 'heed',\n",
       " 'advice',\n",
       " '@flgovscott!\\\\n\\\\n\"if',\n",
       " 'storm',\n",
       " 'enormous',\n",
       " 'destructive',\n",
       " 'power',\n",
       " 'florida-',\n",
       " '\\\\nvisit',\n",
       " 'fi',\n",
       " 'churches',\n",
       " 'texas',\n",
       " 'entitle',\n",
       " 'reimbu',\n",
       " 'message',\n",
       " 'fellow',\n",
       " 'americansðŸ‡ºðŸ‡¸#irmahurrica',\n",
       " 'start',\n",
       " 'reform',\n",
       " 'cut',\n",
       " 'allow',\n",
       " 'pass',\n",
       " 'gr',\n",
       " 'sorry',\n",
       " 'have',\n",
       " 'abou',\n",
       " 'incredible',\n",
       " 'save',\n",
       " 'tha',\n",
       " 'epic',\n",
       " 'proportion',\n",
       " 'confront',\n",
       " 'challenge',\n",
       " 'matter',\n",
       " 'encourage',\n",
       " 'path',\n",
       " 'trip',\n",
       " 'saudi',\n",
       " 'arabia',\n",
       " 'forc',\n",
       " 'welcome',\n",
       " '@whitehouse',\n",
       " 'amir',\n",
       " 'sabah',\n",
       " 'al',\n",
       " 'ahme',\n",
       " 'concern',\n",
       " 'abo',\n",
       " 'honor',\n",
       " 'rag',\n",
       " 'tea',\n",
       " 'company',\n",
       " 'hire',\n",
       " 'amp',\n",
       " 'grow',\n",
       " 'renew',\n",
       " 'prosperity',\n",
       " 'restore',\n",
       " 'op',\n",
       " 'join',\n",
       " 'mandan',\n",
       " 'nd',\n",
       " 'gov.',\n",
       " '@d',\n",
       " 'wonderful',\n",
       " 'incre',\n",
       " 'governors',\n",
       " 'rick',\n",
       " 'scott',\n",
       " 'florida',\n",
       " 'j.',\n",
       " 'approves',\n",
       " 'emergency',\n",
       " 'like',\n",
       " 'large',\n",
       " 'record',\n",
       " 'watch',\n",
       " 'closely',\n",
       " 'team',\n",
       " 'today',\n",
       " 'discuss',\n",
       " 'congress',\n",
       " 'month',\n",
       " 'legalize',\n",
       " 'forward',\n",
       " 'work',\n",
       " 'd',\n",
       " '+',\n",
       " 'r',\n",
       " 'cong',\n",
       " 'obama',\n",
       " 'successor',\n",
       " 'weather',\n",
       " 'afford',\n",
       " 'poll',\n",
       " 'admit',\n",
       " 'divisive',\n",
       " '@potus',\n",
       " 'b',\n",
       " 'nation',\n",
       " 'god',\n",
       " 'indivisible',\n",
       " 'despite',\n",
       " 'immigrant',\n",
       " 'child',\n",
       " 'goodbye',\n",
       " 'p',\n",
       " 'chance',\n",
       " 'comprehensive',\n",
       " 'support',\n",
       " 'pay',\n",
       " 'ea',\n",
       " 'latinos',\n",
       " '17%',\n",
       " 'populat',\n",
       " 'intend',\n",
       " 'begin',\n",
       " 'intruder',\n",
       " 'neighbor',\n",
       " 'o',\n",
       " 'believe',\n",
       " 'heart',\n",
       " 'american',\n",
       " 'live',\n",
       " '@chci',\n",
       " 'tune',\n",
       " 'long',\n",
       " 'overdue',\n",
       " 'good',\n",
       " 'senate',\n",
       " 'appro',\n",
       " 'compare',\n",
       " 'transpare',\n",
       " 'day',\n",
       " 'minute',\n",
       " 'opponent',\n",
       " '@flotus',\n",
       " 'say',\n",
       " 'choice',\n",
       " 'need',\n",
       " 'think',\n",
       " 'marr',\n",
       " 'accuse',\n",
       " 'kind',\n",
       " 'thing',\n",
       " 'youâ€™v',\n",
       " 'old',\n",
       " 'fashioned',\n",
       " 'notion',\n",
       " 'youâ€™',\n",
       " 'â€™m',\n",
       " 'bega',\n",
       " 'high',\n",
       " 'kid',\n",
       " 'safe',\n",
       " 'learn',\n",
       " 'methodist',\n",
       " 'fai',\n",
       " 'try',\n",
       " 'tweet',\n",
       " 'length',\n",
       " 'dtype',\n",
       " 'object']"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(tweets_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = bag_of_words(tweets_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can we clean this up? Need to remove numbers, \\n...urls\n",
    "\n",
    "bow = [re.sub(r'/n','', word) for word in bow]\n",
    "bow = [re.sub(r'\\\\n','', word) for word in bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hmmm... only 206 lemmas kept from 3000 tweets??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u.s.',\n",
       " 'coast',\n",
       " 'guard',\n",
       " 'fema',\n",
       " 'federal',\n",
       " 'heed',\n",
       " 'advice',\n",
       " '@flgovscott!\"if',\n",
       " 'be',\n",
       " 'storm',\n",
       " 'enormous',\n",
       " 'destructive',\n",
       " 'power',\n",
       " 'florida-',\n",
       " 'visit',\n",
       " 'fi',\n",
       " 'churches',\n",
       " 'texas',\n",
       " 'entitle',\n",
       " 'reimbu',\n",
       " 'message',\n",
       " 'fellow',\n",
       " 'americansðŸ‡ºðŸ‡¸#irmahurrica',\n",
       " 'republicans',\n",
       " 'start',\n",
       " 'tax',\n",
       " 'reform',\n",
       " 'tax',\n",
       " 'cut',\n",
       " 'allow',\n",
       " 'republicans',\n",
       " 'pass',\n",
       " 'gr',\n",
       " 'republicans',\n",
       " 'sorry',\n",
       " 'have',\n",
       " 'hear',\n",
       " 'abou',\n",
       " 'incredible',\n",
       " 'u.s.',\n",
       " 'coast',\n",
       " 'guard',\n",
       " 'save',\n",
       " 'tha',\n",
       " 'hurricane',\n",
       " 'irma',\n",
       " 'epic',\n",
       " 'proportion',\n",
       " 'confront',\n",
       " 'challenge',\n",
       " 'matter',\n",
       " 'encourage',\n",
       " 'path',\n",
       " 'hurricane',\n",
       " 'trip',\n",
       " 'saudi',\n",
       " 'arabia',\n",
       " 'speak',\n",
       " 'world',\n",
       " 'forc',\n",
       " 'welcome',\n",
       " '@whitehouse',\n",
       " 'amir',\n",
       " 'sabah',\n",
       " 'al',\n",
       " 'ahme',\n",
       " 'daca',\n",
       " 'concern',\n",
       " 'abo',\n",
       " 'thank',\n",
       " 'great',\n",
       " 'honor',\n",
       " 'hurricane',\n",
       " 'irma',\n",
       " 'rag',\n",
       " 'great',\n",
       " 'tea',\n",
       " 'want',\n",
       " 'company',\n",
       " 'hire',\n",
       " 'amp',\n",
       " 'grow',\n",
       " 'want',\n",
       " 'renew',\n",
       " 'prosperity',\n",
       " 'restore',\n",
       " 'op',\n",
       " 'thank',\n",
       " 'join',\n",
       " 'mandan',\n",
       " 'nd',\n",
       " 'gov.',\n",
       " '@d',\n",
       " 'wonderful',\n",
       " 'north',\n",
       " 'dakota',\n",
       " 'incre',\n",
       " 'speak',\n",
       " 'w/',\n",
       " 'governors',\n",
       " 'rick',\n",
       " 'scott',\n",
       " 'florida',\n",
       " 'president',\n",
       " 'donald',\n",
       " 'j.',\n",
       " 'trump',\n",
       " 'approves',\n",
       " 'emergency',\n",
       " 'hurricane',\n",
       " 'look',\n",
       " 'like',\n",
       " 'large',\n",
       " 'record',\n",
       " 'watch',\n",
       " 'hurricane',\n",
       " 'closely',\n",
       " 'team',\n",
       " 'go',\n",
       " 'north',\n",
       " 'dakota',\n",
       " 'today',\n",
       " 'discuss',\n",
       " 'congress',\n",
       " 'month',\n",
       " 'legalize',\n",
       " 'daca',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'work',\n",
       " 'w/',\n",
       " 'd',\n",
       " \"'s\",\n",
       " '+',\n",
       " 'r',\n",
       " \"'s\",\n",
       " 'cong',\n",
       " 'president',\n",
       " 'obama',\n",
       " 'â€™s',\n",
       " 'successor',\n",
       " 'be',\n",
       " 'weather',\n",
       " 'afford',\n",
       " 't',\n",
       " 'poll',\n",
       " 'close',\n",
       " 'stake',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'refuse',\n",
       " 'admit',\n",
       " 'president',\n",
       " 'trump',\n",
       " 'run',\n",
       " 'divisive',\n",
       " 'campaign',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'refuse',\n",
       " '@potus',\n",
       " 'b',\n",
       " 'america',\n",
       " 'nation',\n",
       " 'god',\n",
       " 'indivisible',\n",
       " 'despite',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'immigrant',\n",
       " 'child',\n",
       " 'goodbye',\n",
       " 'p',\n",
       " 'chance',\n",
       " 'comprehensive',\n",
       " 'support',\n",
       " 'family',\n",
       " 'pay',\n",
       " 'family',\n",
       " 'leave',\n",
       " 'ea',\n",
       " 'be',\n",
       " 'wrong',\n",
       " 'latinos',\n",
       " '17%',\n",
       " 'populat',\n",
       " 'intend',\n",
       " 'close',\n",
       " 'campaign',\n",
       " 'way',\n",
       " 'begin',\n",
       " 'â€™re',\n",
       " 'intruder',\n",
       " 'â€™re',\n",
       " 'neighbor',\n",
       " 'o',\n",
       " 'believe',\n",
       " 'heart',\n",
       " 'american',\n",
       " 'hillary',\n",
       " 'â€™s',\n",
       " 'live',\n",
       " '@chci',\n",
       " 'tune',\n",
       " 'hear',\n",
       " 'long',\n",
       " 'overdue',\n",
       " 'good',\n",
       " 'senate',\n",
       " 'appro',\n",
       " 'want',\n",
       " 'compare',\n",
       " 'hillary',\n",
       " 'trump',\n",
       " 'transpare',\n",
       " 'day',\n",
       " 'leave',\n",
       " 'not',\n",
       " 'minute',\n",
       " 't',\n",
       " 'be',\n",
       " 'opponent',\n",
       " 'america',\n",
       " 'wrong',\n",
       " 'â€™s',\n",
       " '@flotus',\n",
       " 'say',\n",
       " 'choice',\n",
       " 'election',\n",
       " 'not',\n",
       " 'need',\n",
       " 'president',\n",
       " 'think',\n",
       " 'marr',\n",
       " 'people',\n",
       " 'accuse',\n",
       " 'kind',\n",
       " 'thing',\n",
       " 'youâ€™v',\n",
       " 'old',\n",
       " 'fashioned',\n",
       " 'notion',\n",
       " 'youâ€™',\n",
       " 'â€™m',\n",
       " 'go',\n",
       " 'close',\n",
       " 'campaign',\n",
       " 'way',\n",
       " 'bega',\n",
       " 'stake',\n",
       " 'election',\n",
       " 'high',\n",
       " 'want',\n",
       " 'kid',\n",
       " 'safe',\n",
       " 'world',\n",
       " 'learn',\n",
       " 'family',\n",
       " 'methodist',\n",
       " 'fai',\n",
       " 'hillary',\n",
       " 'â€™s',\n",
       " 'run',\n",
       " 'people',\n",
       " 'try',\n",
       " 'tweet',\n",
       " 'length',\n",
       " 'dtype',\n",
       " 'object']"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some of these still don't look quite right, like '@flgovscott!\"if'\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TOPIC Identification via LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "#https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730\n",
    "#https://www.youtube.com/watch?v=3mHy4OSyRf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "#Any idea on whether this dataset is topic appropriate / large enough to train LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "1. Walkthrough for creating password file and accessing at begining\n",
    "2. Improving BoW output/fixing partial words\n",
    "\n",
    "\n",
    "3. Capstone instructions: \n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "        --What is meant by clusters here? \n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "        --Can include features (topics) generated by LSA/LDA which are unsupervised features\n",
    "        --How would we use an unsupervised model to classify in this case? Would it be possible to use K-Means, for example,\n",
    "        on the LSA or LDA matrix to cluster?\n",
    "\n",
    "\n",
    "\n",
    "General question: how would you go about creating a model to optimize something? Like if I wanted to optimize profitability and my goal was to choose the optimal credit line for each customer?\n",
    "\n",
    "If we have time: Stupid question, but can demo how to run this in another format, like in terminal or spyder?\n",
    "\n",
    "What are text editors like Atom useful for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
