{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras for Neural Networks - Guided Example\n",
    "\n",
    "Here we're going to work through a classic Machine Learning problem - digit recognition. This data is referred to as the MNIST dataset, which stands for Modified National Institute of Standards and Technology, and represents probably the most used dataset in the world for advanced machine learning techniques (though the iris dataset would be a close second). Here we're forgoing a more business focussed dataset for a few reasons. Firstly, this dataset is the most written about dataset in these topics - you'll easily find other guides using pure TensorFlow or other tools like Theano to solve the same problem with the same class of models. Similarly, you can also easily find several different kinds of neural networks being used to solve this problem. This will be valuable as you try to expand your knowledge of different kinds of layers and combinations.\n",
    "\n",
    "We'll be building our code off of the examples provided in the Keras documentation, and found in full on its [creator's github](https://github.com/fchollet/keras/tree/master/examples). \n",
    "\n",
    "Our goal here will be simple but multifaceted. Overall we are going to use the MNIST dataset and neural networks to classify handwritten numbers as the proper digits. This will be thought of as a multi-class classification problem, specifically with 10 classes (one for each possible digit).\n",
    "\n",
    "However, we will use this to teach a few new kinds of neural network compositions, creating three different styles of network and discussing their relative advantages and disadvantages. Through this we will delve a little deeper into neural network theory.\n",
    "\n",
    "But before we go too far, let's actually look at the data.\n",
    "\n",
    "## MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at this data you'll notice its organization structure is not images. We don't actually see any pictures of digits here. Instead, what we have is values of pixels, a simple way of converting images into numeric data on which we can train a model.\n",
    "\n",
    "However, this still doesn't look like most of the data we've worked with previously. It's not a single table, but rather a different, higher dimensionality structure. It is often described as a set of clouds, each cloud representing an image. The cloud contains columns of values, representing the darkness of pixels. That's great, but not an easy or meaningful dataset on which to directly train a model. The darkness of the second pixel in the third column isn't likely linearly related to likelihood the cloud represents a certain digit. Instead, we need to find meaningful patterns within our clouds, creating models off of those patterns.\n",
    "\n",
    "This is exactly what neural networks are good at. Multiple layers will allow us to transform this clouds full of values into meaningful vectors containing the information we need to be able to create a model, admittedly in an unlabeled and unsupervised fashion. Our output, however, will be labels for each of the clouds, giving us predictions as to what digit they are meant to represent.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron\n",
    "\n",
    "Let's start with a kind of neural network we've seen before: a multi-layer perceptron. Recall from our previous neural networks sections that this is a set of perceptron models organized into layers, one layer feeding into the next.\n",
    "\n",
    "To do this, we will first need to reshape our data into flat vectors for each digit. We'll also need to convert our outcome to a matrix of binary variables, rather than the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Change shape \n",
    "# Note that our images are 28*28 pixels, so in reshaping to arrays we want\n",
    "# 60,000 arrays of length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we can create our model. We'll do this using dense layers and dropouts. Dense layers are simply fully connected layers with a given number of perceptrons. Dropout drops a certain portion of our perceptrons in order to prevent overfitting. Our activation function, `relu` stands for Rectified Linear Unit, which is standard but can be read about more [here](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model. This we can use to accomplish our wildest dreams of data modeling, or at least predict some digits from pixel data. To do that we will use epochs, effectively iterations of the model, improving based on what it learned previously. Batch size is the number of samples to use in each step improving the model and will affect speed, but also slightly negatively impact accuracy (learning in bigger steps will affect what your model learns).\n",
    "\n",
    "Note that we are going with 64 perceptron wide layers, this is relatively arbitrary, though units within the $2^x$ series will parallelize more efficiently. Also note that our number of parameters is the product of our input width plus one and our layer width. This reflects the number of weights we're creating in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.4406 - acc: 0.8729 - val_loss: 0.1924 - val_acc: 0.9428\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.2081 - acc: 0.9381 - val_loss: 0.1394 - val_acc: 0.9584\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.1577 - acc: 0.9530 - val_loss: 0.1121 - val_acc: 0.9654\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.1307 - acc: 0.9597 - val_loss: 0.1038 - val_acc: 0.9683\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.1150 - acc: 0.9653 - val_loss: 0.1027 - val_acc: 0.9697\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.1016 - acc: 0.9695 - val_loss: 0.0898 - val_acc: 0.9736\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0954 - acc: 0.9712 - val_loss: 0.0893 - val_acc: 0.9743\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0864 - acc: 0.9734 - val_loss: 0.0830 - val_acc: 0.9756\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0828 - acc: 0.9745 - val_loss: 0.0907 - val_acc: 0.9741\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0783 - acc: 0.9762 - val_loss: 0.0839 - val_acc: 0.9759\n",
      "Test loss: 0.0838920256578\n",
      "Test accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did impressively well for such a simple neural network, with each epoch training in about 1 second on this machine and giving us an accuracy in the high 90's. But what else can we do? Let's let our model get much more complicated by introducting convolution.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "Before we go any further, do you recall that we've talked about how complex neural networks can get, and the degree of computational complexity that entails? Well, here we're going to finally truly experience that complexity, so be careful about rerunning this code. It will take some serious time (potentially on the order of hours) to run.\n",
    "\n",
    "Now that that's out of the way, let's talk convolution. First, a simple definition. Convolution basically takes your data and creates overlapping subsegments testing for a given feature in a set of spaces and upon which it develops its model.\n",
    "\n",
    "Let's extend that definition since it's incredibly dense.\n",
    "\n",
    "First, you have to define a shape of your input data. This can theoretically be in any number of dimensions, though for our image example we will use 2d, since images are in two dimensions. This is also why you'll see 2D in some of our layer definitions (though more on that later). Our first chunk of code after loading the data does this reshaping (with a conditional on the data format).\n",
    "\n",
    "Over that shaped data, we then create our tiles, also called __kernels__. These kernels are like little windows, that will look over subsets of the data of a given size. In the example below we create 3x3 kernels, which run overlapping over the whole 28x28 input looking for features. That is the convolutional layer, a way of searching for a subpattern over the whole of the image. We can chain multiple of these convolutional layers together, with the below example having two.\n",
    "\n",
    "Next comes a pooling layer. This is a _downsampling_ technique, which effectively serves to reduce sample size and simplify later processes. For each value generated by our convolutional layers, it looks over the grid in _non_-overlapping segments and takes the maximum value of those outputs. It's not the feautres exact location then that matters, but its approximate or relative location. After pooling you will want to flatten the data back out, so that it can be put into dense layers as we did in MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 136s - loss: 0.3385 - acc: 0.8975 - val_loss: 0.0842 - val_acc: 0.9735\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 136s - loss: 0.1178 - acc: 0.9649 - val_loss: 0.0551 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 130s - loss: 0.0891 - acc: 0.9736 - val_loss: 0.0483 - val_acc: 0.9845\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 144s - loss: 0.0746 - acc: 0.9784 - val_loss: 0.0394 - val_acc: 0.9866\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 137s - loss: 0.0639 - acc: 0.9803 - val_loss: 0.0369 - val_acc: 0.9879\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 138s - loss: 0.0577 - acc: 0.9825 - val_loss: 0.0336 - val_acc: 0.9879\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 145s - loss: 0.0530 - acc: 0.9837 - val_loss: 0.0361 - val_acc: 0.9870\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 137s - loss: 0.0477 - acc: 0.9858 - val_loss: 0.0308 - val_acc: 0.9895\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 139s - loss: 0.0438 - acc: 0.9869 - val_loss: 0.0300 - val_acc: 0.9896\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 141s - loss: 0.0437 - acc: 0.9868 - val_loss: 0.0294 - val_acc: 0.9902\n",
      "Test loss: 0.0294480922608\n",
      "Test accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is incredibly impressive accuracy. 99% is really exceptional, but it did take a long time to get there. Such are the costs of convolution.\n",
    "\n",
    "There is one more classic construction of a neural network: Recurrent, which we'll give quick mention.\n",
    "\n",
    "## Hierarchical Recurrrent Neural Networks\n",
    "\n",
    "So far when we've talked about neural networks we've talked about them as feedforward: data flows in one direction until it reaches the end. Recurrent neural networks do not obey that directional logic, instead letting the data cycle through the network.\n",
    "\n",
    "However, to do this we have to abandon the sequential model building we've done so far and things can get much more complicated. You have to use recurrent layers and often time distribution (which handles the extra dimension created through the LTSM layer, as a time dimension) to get these things running. You can find an example of a hierarchical recurrent network below (via the link [here](https://github.com/fchollet/keras/blob/master/examples/mnist_hierarchical_rnn.py)). When you get comfortable with networks as they exist in Keras for both convolution and MLP, start exploring recurrence. Note that this will take an even longer time than the previous ones should you choose to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 352s - loss: 0.9851 - acc: 0.6618 - val_loss: 0.5004 - val_acc: 0.8216\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 350s - loss: 0.4009 - acc: 0.8709 - val_loss: 0.3192 - val_acc: 0.8997\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 346s - loss: 0.2343 - acc: 0.9284 - val_loss: 0.2199 - val_acc: 0.9293\n",
      "Test loss: 0.219856005466\n",
      "Test accuracy: 0.9293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be comfortable building some neural networks, but let's see if you can improve them!\n",
    "\n",
    "# Drill: 99% MLP\n",
    "\n",
    "We have the MLP above, which runs reasonably quickly. Copy that code down here and see if you can tune it to achieve 99% accuracy with a Multi-Layer Perceptron. Does it run faster than the recurrent or concolutional neural nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decreasing batch size to 64 helped to converge on a solution faster. Batches of 20, 32 seemed to be too small\n",
    "#Striking a balance is difficult but important\n",
    "\n",
    "#Also tried altering the loss model, the optimizer, number of layers, number of neurons in each layer, \n",
    "#Dropout, activation function of the neurons, and learning rate\n",
    "\n",
    "#This code runs slower than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 950,554\n",
      "Trainable params: 950,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1235 - acc: 0.9295 - val_loss: 0.1115 - val_acc: 0.9646\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 49s - loss: 0.1083 - acc: 0.9752 - val_loss: 0.1071 - val_acc: 0.9775\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1054 - acc: 0.9833 - val_loss: 0.1069 - val_acc: 0.9790\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1039 - acc: 0.9876 - val_loss: 0.1067 - val_acc: 0.9795\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1029 - acc: 0.9912 - val_loss: 0.1068 - val_acc: 0.9805\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 54s - loss: 0.1023 - acc: 0.9927 - val_loss: 0.1067 - val_acc: 0.9812\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1017 - acc: 0.9948 - val_loss: 0.1063 - val_acc: 0.9834\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1014 - acc: 0.9960 - val_loss: 0.1071 - val_acc: 0.9830\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1010 - acc: 0.9966 - val_loss: 0.1065 - val_acc: 0.9838\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1010 - acc: 0.9969 - val_loss: 0.1067 - val_acc: 0.9843\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1008 - acc: 0.9976 - val_loss: 0.1068 - val_acc: 0.9845\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1006 - acc: 0.9981 - val_loss: 0.1067 - val_acc: 0.9845\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1005 - acc: 0.9987 - val_loss: 0.1074 - val_acc: 0.9836\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1005 - acc: 0.9984 - val_loss: 0.1071 - val_acc: 0.9846\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1004 - acc: 0.9988 - val_loss: 0.1072 - val_acc: 0.9840\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1004 - acc: 0.9989 - val_loss: 0.1076 - val_acc: 0.9850\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1004 - acc: 0.9989 - val_loss: 0.1079 - val_acc: 0.9842\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1003 - acc: 0.9991 - val_loss: 0.1076 - val_acc: 0.9838\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1003 - acc: 0.9991 - val_loss: 0.1079 - val_acc: 0.9837\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1003 - acc: 0.9990 - val_loss: 0.1075 - val_acc: 0.9841\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9995 - val_loss: 0.1076 - val_acc: 0.9844\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9996 - val_loss: 0.1080 - val_acc: 0.9845\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9994 - val_loss: 0.1084 - val_acc: 0.9839\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9995 - val_loss: 0.1086 - val_acc: 0.9842\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9994 - val_loss: 0.1084 - val_acc: 0.9846\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1002 - acc: 0.9994 - val_loss: 0.1079 - val_acc: 0.9844\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9994 - val_loss: 0.1087 - val_acc: 0.9831\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1003 - acc: 0.9991 - val_loss: 0.1081 - val_acc: 0.9841\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9994 - val_loss: 0.1079 - val_acc: 0.9854\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1080 - val_acc: 0.9850\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9994 - val_loss: 0.1083 - val_acc: 0.9844\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1080 - val_acc: 0.9846\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9995 - val_loss: 0.1086 - val_acc: 0.9841\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1086 - val_acc: 0.9839\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1083 - val_acc: 0.9849\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1084 - val_acc: 0.9853\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1087 - val_acc: 0.9844\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1086 - val_acc: 0.9846\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1086 - val_acc: 0.9851\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1088 - val_acc: 0.9850\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1002 - acc: 0.9995 - val_loss: 0.1085 - val_acc: 0.9849\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1084 - val_acc: 0.9857\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1085 - val_acc: 0.9854\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1086 - val_acc: 0.9845\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1085 - val_acc: 0.9846\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1083 - val_acc: 0.9848\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 56s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1084 - val_acc: 0.9847\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1084 - val_acc: 0.9853\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.1085 - val_acc: 0.9856\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1087 - val_acc: 0.9849\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9998 - val_loss: 0.1082 - val_acc: 0.9857\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1086 - val_acc: 0.9848\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1086 - val_acc: 0.9857\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1087 - val_acc: 0.9850\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 55s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1087 - val_acc: 0.9857\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 53s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1087 - val_acc: 0.9851\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1086 - val_acc: 0.9856\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1085 - val_acc: 0.9856\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1089 - val_acc: 0.9859\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1088 - val_acc: 0.9859\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1088 - val_acc: 0.9856\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1087 - val_acc: 0.9858\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1087 - val_acc: 0.9843\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1089 - val_acc: 0.9852\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1085 - val_acc: 0.9855\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1086 - val_acc: 0.9857\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1001 - acc: 0.9997 - val_loss: 0.1094 - val_acc: 0.9845\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1092 - val_acc: 0.9852\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1090 - val_acc: 0.9857\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1092 - val_acc: 0.9856\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1093 - val_acc: 0.9850\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1089 - val_acc: 0.9854\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1089 - val_acc: 0.9852\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1091 - val_acc: 0.9854\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.1091 - val_acc: 0.9851\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1090 - val_acc: 0.9849\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1089 - val_acc: 0.9851\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1088 - val_acc: 0.9856\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9857\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9855\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1087 - val_acc: 0.9859\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 0.9857\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1087 - val_acc: 0.9853\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9861\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1086 - val_acc: 0.9860\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1001 - acc: 0.9998 - val_loss: 0.1086 - val_acc: 0.9866\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9998 - val_loss: 0.1085 - val_acc: 0.9865\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 0.9855\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9854\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1088 - val_acc: 0.9866\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9998 - val_loss: 0.1090 - val_acc: 0.9854\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9864\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9998 - val_loss: 0.1090 - val_acc: 0.9861\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 50s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9860\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1088 - val_acc: 0.9859\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 51s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9863\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9870\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1088 - val_acc: 0.9862\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1091 - val_acc: 0.9854\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 52s - loss: 0.1000 - acc: 0.9999 - val_loss: 0.1089 - val_acc: 0.9869\n",
      "Test loss: 0.108898254478\n",
      "Test accuracy: 0.9869\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Import the dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Change shape \n",
    "# Note that our images are 28*28 pixels, so in reshaping to arrays we want\n",
    "# 60,000 arrays of length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(784, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='poisson',\n",
    "              optimizer=keras.optimizers.Adagrad(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
