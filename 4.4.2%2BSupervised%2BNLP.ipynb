{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')  Carroll\n",
       "4  ((, when, she, thought, it, over, afterwards, ...  Carroll"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anew</th>\n",
       "      <th>hatter</th>\n",
       "      <th>prevent</th>\n",
       "      <th>snappishly</th>\n",
       "      <th>crawl</th>\n",
       "      <th>cheek</th>\n",
       "      <th>domestic</th>\n",
       "      <th>get</th>\n",
       "      <th>baldwin</th>\n",
       "      <th>lowing</th>\n",
       "      <th>...</th>\n",
       "      <th>consider</th>\n",
       "      <th>sink</th>\n",
       "      <th>dismay</th>\n",
       "      <th>remembrance</th>\n",
       "      <th>back</th>\n",
       "      <th>bit</th>\n",
       "      <th>bell</th>\n",
       "      <th>condemn</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  anew hatter prevent snappishly crawl cheek domestic get baldwin lowing  \\\n",
       "0    0      0       0          0     0     0        0   0       0      0   \n",
       "1    0      0       0          0     0     0        0   1       0      0   \n",
       "2    0      0       0          0     0     0        0   0       0      0   \n",
       "3    0      0       0          0     0     0        0   0       0      0   \n",
       "4    0      0       0          0     0     0        0   0       0      0   \n",
       "\n",
       "      ...     consider sink dismay remembrance back bit bell condemn  \\\n",
       "0     ...            0    0      0           0    0   0    0       0   \n",
       "1     ...            1    0      0           0    0   0    0       0   \n",
       "2     ...            0    0      0           0    0   0    0       0   \n",
       "3     ...            0    0      0           0    0   0    0       0   \n",
       "4     ...            0    0      0           0    0   0    0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')     Carroll  \n",
       "4  ((, when, she, thought, it, over, afterwards, ...     Carroll  \n",
       "\n",
       "[5 rows x 3025 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.992290748899\n",
      "\n",
      "Test set score: 0.90859030837\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n",
    "\n",
    "\n",
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2724, 3023) (2724,)\n",
      "Training set score: 0.963656387665\n",
      "\n",
      "Test set score: 0.935572687225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.919970631424\n",
      "\n",
      "Test set score: 0.908039647577\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "# Emma is quite long, let's cut it down to the same length as Alice.\n",
    "emma_sents = emma_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.703466666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1158</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>535</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1158       21\n",
       "Carroll     535      161"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 97%.  \n",
    "\n",
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.734030837004\n"
     ]
    }
   ],
   "source": [
    "#Before trying anything else, let's try a SVC model to see if we can beat 93% from logistic...\n",
    "\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Score:\", svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try adding some common phrases (500 from each author) from the works into our dataframe\n",
    "\n",
    "\n",
    "def text_phrases(text):\n",
    "    noun_phrases = [np.text for np in text.noun_chunks]\n",
    "    return [item[0] for item in Counter(noun_phrases).most_common(500)]\n",
    "\n",
    "\n",
    "# Set up\n",
    "alice_phrases = text_phrases(alice_doc)\n",
    "persuasion_phrases = text_phrases(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_phrases = set(alice_phrases + persuasion_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the original code I have now added: number of words in sentence, num words in prior and next sentences, number of repeated words from last \n",
    "sentence, numbers of different parts of speech, numbers of different entity types, amount of punctuation, and phrases\n",
    "\n",
    "Also added a feature to indicate presence of a word in ALL CAPS (seems to be part of Carroll's style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(4000)]\n",
    "    \n",
    "\n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "\n",
    "    #Get a list of all possible parts of speech and entities \n",
    "    #and attach this to our dataframe scaffold/structure, so that we can get counts\n",
    "\n",
    "    partsofspeechlist = []\n",
    "    ent_list = []\n",
    "\n",
    "    for sent in range(len(sentences)):\n",
    "        for tok in range(len(sentences[0][sent])):\n",
    "            partsofspeechlist.append(sentences[0][sent][tok].pos_)\n",
    "            ent_list.append(sentences[0][sent][tok].ent_type_)\n",
    "\n",
    "    parts = (set(partsofspeechlist))\n",
    "    ents = (set(ent_list))\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=set(list(common_words) + list(parts) + list(ents) + list(common_phrases)))\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    df['sent_length'] = 0\n",
    "    df['sent_punct_count'] = 0 \n",
    "    df['prev_sent_length'] = 0\n",
    "    df['next_sent_length'] = np.nan\n",
    "    df['num_words_repeated_from_prior_sent'] = 0\n",
    "    df['upper_case'] = 0 \n",
    "    df.loc[:, parts] = 0\n",
    "    df.loc[:, ents] = 0\n",
    "    df.loc[:, common_phrases] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        \n",
    "        #Check to see if each phrase turns up in the sentence (store as binary var for the time being)\n",
    "        \n",
    "        for phrase in common_phrases:\n",
    "            if phrase in str(sentence):\n",
    "                df.loc[i, phrase] = 1\n",
    "        \n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        \n",
    "        #Also add # of repeated words from one sentence to the next\n",
    "        repeats = 0\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "            if i > 0: \n",
    "                if ((df.loc[i-1, word] > 0) & (df.loc[i, word] > 0)):\n",
    "                    repeats += 1\n",
    "            else: \n",
    "                repeats = np.nan\n",
    "        df['num_words_repeated_from_prior_sent'][i] = repeats\n",
    "        \n",
    "        #Add part of speech counter\n",
    "        \n",
    "        for token in sentence:\n",
    "            df.loc[i, token.pos_] += 1\n",
    "            df.loc[i, token.ent_type_] += 1\n",
    "            \n",
    "        #I also noticed that Carroll tends to use a lot of ALL UPPER CASE words, so I'll create a feature for that as well    \n",
    "            if ((str(token).isupper()) & (len(str(token)) > 1)):\n",
    "                df.loc[i, 'upper_case'] = 1\n",
    "\n",
    "#My code -- calculate sentence length and amount of punctuation!\n",
    "\n",
    "        sent_len = 0    \n",
    "        num_punct = 0 \n",
    "        \n",
    "        for token in sentence:\n",
    "            if not token.is_punct:\n",
    "                sent_len += 1\n",
    "            else:\n",
    "                num_punct += 1\n",
    "        df.loc[i, 'sent_length'] = sent_len\n",
    "        df.loc[i, 'sent_punct_count'] = num_punct\n",
    "        \n",
    "        if i > 0:\n",
    "            df.loc[i, 'prev_sent_length'] = df.loc[i-1, 'sent_length']\n",
    "        else:\n",
    "            df.loc[i, 'prev_sent_length'] = np.nan\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "    \n",
    "    #Back out of the loop through sentences and just shift the df by one to get the \"next sent len\" feature\n",
    "    df['next_sent_length'] = df['sent_length'].shift(-1) \n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>squareness</th>\n",
       "      <th>the Musgroves</th>\n",
       "      <th>excited</th>\n",
       "      <th>a few hours</th>\n",
       "      <th>the dance</th>\n",
       "      <th>greatly</th>\n",
       "      <th>property</th>\n",
       "      <th>pigs</th>\n",
       "      <th>transmit</th>\n",
       "      <th>...</th>\n",
       "      <th>bell</th>\n",
       "      <th>condemn</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>sent_punct_count</th>\n",
       "      <th>prev_sent_length</th>\n",
       "      <th>next_sent_length</th>\n",
       "      <th>num_words_repeated_from_prior_sent</th>\n",
       "      <th>upper_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>109</td>\n",
       "      <td>17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       squareness  the Musgroves excited  a few hours  the dance greatly  \\\n",
       "0   65          0              0       0            0          0       0   \n",
       "1   61          0              0       0            0          0       0   \n",
       "2   29          0              0       0            0          0       0   \n",
       "3    9          0              0       0            0          0       0   \n",
       "4  122          0              0       0            0          0       0   \n",
       "\n",
       "  property  pigs transmit    ...     bell condemn  \\\n",
       "0        0     0        0    ...        0       0   \n",
       "1        0     0        0    ...        0       0   \n",
       "2        0     0        0    ...        0       0   \n",
       "3        0     0        0    ...        0       0   \n",
       "4        0     0        0    ...        0       0   \n",
       "\n",
       "                                       text_sentence text_source sent_length  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll          57   \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll          56   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll          29   \n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')     Carroll           6   \n",
       "4  ((, when, she, thought, it, over, afterwards, ...     Carroll         109   \n",
       "\n",
       "   sent_punct_count  prev_sent_length next_sent_length  \\\n",
       "0                10               NaN             56.0   \n",
       "1                 7              57.0             29.0   \n",
       "2                 4              56.0              6.0   \n",
       "3                 3              29.0            109.0   \n",
       "4                17               6.0             21.0   \n",
       "\n",
       "   num_words_repeated_from_prior_sent upper_case  \n",
       "0                                 NaN          0  \n",
       "1                                 0.0          0  \n",
       "2                                 1.0          1  \n",
       "3                                 2.0          0  \n",
       "4                                 0.0          1  \n",
       "\n",
       "[5 rows x 5539 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rebuild the word_counts df with our new features\n",
    "\n",
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PART</th>\n",
       "      <th>DET</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>VERB</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOUN  X  PUNCT  NUM  ADV  PRON  PART  DET  CCONJ  ADJ  ADP  VERB  INTJ  \\\n",
       "0    12  0      9    0    3     3     3    5      6    3    8    13     0   \n",
       "1     8  0      7    0    7     4     1    6      2    7    8    11     0   \n",
       "2     2  0      4    0    1     2     1    3      1    2    6     5     2   \n",
       "3     0  0      3    0    0     1     0    0      0    1    0     2     2   \n",
       "4    15  0     17    0   11    13     5   11      8    5   18    19     0   \n",
       "\n",
       "   PROPN   Source  \n",
       "0      2  Carroll  \n",
       "1      2  Carroll  \n",
       "2      4  Carroll  \n",
       "3      0  Carroll  \n",
       "4      4  Carroll  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a look at how the parts of speech compared between these two authors - step 1\n",
    "\n",
    "viz_df = pd.DataFrame(word_counts.loc[:, parts])\n",
    "viz_df['Source'] = word_counts['text_source']\n",
    "viz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PART</th>\n",
       "      <th>DET</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>VERB</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>PROPN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>0.111198</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.206852</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.077693</td>\n",
       "      <td>0.082409</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>0.084796</td>\n",
       "      <td>0.032923</td>\n",
       "      <td>0.060635</td>\n",
       "      <td>0.087940</td>\n",
       "      <td>0.181789</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.039356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>0.128021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144349</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.074930</td>\n",
       "      <td>0.074065</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.069938</td>\n",
       "      <td>0.038827</td>\n",
       "      <td>0.090861</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>0.182556</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.050926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             NOUN         X     PUNCT       NUM       ADV      PRON      PART  \\\n",
       "Source                                                                          \n",
       "Carroll  0.111198  0.000029  0.206852  0.005764  0.077693  0.082409  0.023986   \n",
       "Austen   0.128021  0.000000  0.144349  0.005388  0.074930  0.074065  0.026129   \n",
       "\n",
       "              DET     CCONJ       ADJ       ADP      VERB      INTJ     PROPN  \n",
       "Source                                                                         \n",
       "Carroll  0.084796  0.032923  0.060635  0.087940  0.181789  0.004628  0.039356  \n",
       "Austen   0.069938  0.038827  0.090861  0.111723  0.182556  0.002288  0.050926  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2\n",
    "\n",
    "carroll_df = viz_df[viz_df['Source'] == 'Carroll'].groupby('Source').sum() / (len(alice_doc))\n",
    "austen_df = viz_df[viz_df['Source'] == 'Austen'].groupby('Source').sum() / (len(persuasion_doc))\n",
    "new_df = carroll_df.append(austen_df)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27eb7fa1470>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEUCAYAAAAr20GQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27e834d7780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the parts of speech distribution among each author\n",
    "\n",
    "new_df.T.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  1 Logistic Train Score: 0.980971972229\n",
      "Test:  1 Logistic Test Score: 0.947611710324\n",
      "\n",
      "Test:  1 RFC Train Score: 0.989714579583\n",
      "Test:  1 RFC Test Score: 0.902927580894\n",
      "\n",
      "Test:  1 GBM Train Score: 0.933916173824\n",
      "Test:  1 GBM Test Score: 0.922958397535\n",
      "\n",
      "Test:  2 Logistic Train Score: 0.979686294677\n",
      "Test:  2 Logistic Test Score: 0.956856702619\n",
      "\n",
      "Test:  2 RFC Train Score: 0.993314476729\n",
      "Test:  2 RFC Test Score: 0.906009244992\n",
      "\n",
      "Test:  2 GBM Train Score: 0.930830547699\n",
      "Test:  2 GBM Test Score: 0.907550077042\n",
      "\n",
      "Test:  3 Logistic Train Score: 0.980205655527\n",
      "Test:  3 Logistic Test Score: 0.952160493827\n",
      "\n",
      "Test:  3 RFC Train Score: 0.995372750643\n",
      "Test:  3 RFC Test Score: 0.912037037037\n",
      "\n",
      "Test:  3 GBM Train Score: 0.933161953728\n",
      "Test:  3 GBM Test Score: 0.916666666667\n",
      "\n",
      "Test:  4 Logistic Train Score: 0.982005141388\n",
      "Test:  4 Logistic Test Score: 0.927469135802\n",
      "\n",
      "Test:  4 RFC Train Score: 0.991516709512\n",
      "Test:  4 RFC Test Score: 0.904320987654\n",
      "\n",
      "Test:  4 GBM Train Score: 0.933676092545\n",
      "Test:  4 GBM Test Score: 0.901234567901\n",
      "\n",
      "Test:  5 Logistic Train Score: 0.981748071979\n",
      "Test:  5 Logistic Test Score: 0.942901234568\n",
      "\n",
      "Test:  5 RFC Train Score: 0.993316195373\n",
      "Test:  5 RFC Test Score: 0.902777777778\n",
      "\n",
      "Test:  5 GBM Train Score: 0.933933161954\n",
      "Test:  5 GBM Test Score: 0.91975308642\n",
      "\n",
      "Test:  6 Logistic Train Score: 0.980976863753\n",
      "Test:  6 Logistic Test Score: 0.944444444444\n",
      "\n",
      "Test:  6 RFC Train Score: 0.994601542416\n",
      "Test:  6 RFC Test Score: 0.922839506173\n",
      "\n",
      "Test:  6 GBM Train Score: 0.931362467866\n",
      "Test:  6 GBM Test Score: 0.930555555556\n",
      "\n",
      "Test:  7 Logistic Train Score: 0.982005141388\n",
      "Test:  7 Logistic Test Score: 0.933641975309\n",
      "\n",
      "Test:  7 RFC Train Score: 0.994087403599\n",
      "Test:  7 RFC Test Score: 0.916666666667\n",
      "\n",
      "Test:  7 GBM Train Score: 0.932904884319\n",
      "Test:  7 GBM Test Score: 0.918209876543\n",
      "\n",
      "Average LR Score: 0.943583670985\n",
      "Average RFC Score: 0.909654114456\n",
      "Average GBM Score: 0.916704032523\n"
     ]
    }
   ],
   "source": [
    "#Do not include SVC because it takes forever to run and it has poor performance\n",
    "X2 = word_counts.dropna()\n",
    "\n",
    "Y = np.array(X2['text_source'])\n",
    "X = np.array(X2.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "#Test accuracy using k-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lrscore = []\n",
    "rfcscore = []\n",
    "gbmscore = []\n",
    "svcscore = []\n",
    "\n",
    "testno = 1\n",
    "\n",
    "kf = KFold(n_splits=7, shuffle = True)\n",
    "for train_index, test_index in kf.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    train = lr.fit(X_train, y_train)\n",
    "    print('Test: ', testno, 'Logistic Train Score:', lr.score(X_train, y_train))\n",
    "    print('Test: ', testno, 'Logistic Test Score:', lr.score(X_test, y_test))\n",
    "    lrscore.append(lr.score(X_test, y_test))\n",
    "    print('')\n",
    "\n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    train = rfc.fit(X_train, y_train)\n",
    "    print('Test: ', testno, 'RFC Train Score:', rfc.score(X_train, y_train))\n",
    "    print('Test: ', testno, 'RFC Test Score:', rfc.score(X_test, y_test))\n",
    "    rfcscore.append(rfc.score(X_test, y_test))\n",
    "    print('')\n",
    "    \n",
    "    clf = ensemble.GradientBoostingClassifier()\n",
    "    train = clf.fit(X_train, y_train)\n",
    "    print('Test: ', testno, 'GBM Train Score:', clf.score(X_train, y_train))\n",
    "    print('Test: ', testno, 'GBM Test Score:', clf.score(X_test, y_test))\n",
    "    gbmscore.append(clf.score(X_test, y_test))\n",
    "    print('')\n",
    "    \n",
    "    testno += 1\n",
    "print('Average LR Score:', np.mean(lrscore))\n",
    "print('Average RFC Score:', np.mean(rfcscore))\n",
    "print('Average GBM Score:', np.mean(gbmscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try optimizing the RFC model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X2 = word_counts.dropna()\n",
    "\n",
    "Y = np.array(X2['text_source'])\n",
    "X = np.array(X2.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "# Split the dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30)\n",
    "\n",
    "tuned_parameters = [{'penalty': ['l1', 'l2'],\n",
    "                     'class_weight': ['balanced', None],\n",
    "                     'C': [0.1, 1, 10, 100, 1000]}]\n",
    " \n",
    "gscv = GridSearchCV(estimator = LogisticRegression(), param_grid = tuned_parameters, cv = 7, n_jobs = -1)\n",
    "gscv.fit(X_train, y_train)\n",
    "gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Score: 0.941435768262\n"
     ]
    }
   ],
   "source": [
    "print('Best Logistic Regression Score:', gscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try optimizing the RFC model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X2 = word_counts.dropna()\n",
    "\n",
    "Y = np.array(X2['text_source'])\n",
    "X = np.array(X2.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "# Split the dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30)\n",
    "\n",
    "tuned_parameters = [{'n_estimators': [25, 100],\n",
    "                     'max_features': [1, 3, None],\n",
    "                     'max_depth': [1, 2, 5, None],\n",
    "                     'criterion': ['gini', 'entropy']}]\n",
    " \n",
    "gscv = GridSearchCV(estimator = ensemble.RandomForestClassifier(), param_grid = tuned_parameters, cv = 3, n_jobs = -1)\n",
    "gscv.fit(X_train, y_train)\n",
    "gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RFC Score: 0.923173803526\n"
     ]
    }
   ],
   "source": [
    "print('Best RFC Score:', gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models did not really improve very much - at least not our best model (logistic regression) - was not able to achieve\n",
    "97% accuracy per the challenge.\n",
    "\n",
    "Challenge mentions checking grammar. How would I go about doing that?\n",
    "How can we leverage the dependency parser to improve our model?\n",
    "\n",
    "Any other ideas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "#Challenge part 2: Incorporate another work. We'll see how well the model does in distinguishing between 3 authors.\n",
    "\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in and clean a new book, using our text_cleaner function and the spacy load function\n",
    "paradise = gutenberg.raw('milton-paradise.txt')\n",
    "paradise = text_cleaner(paradise)\n",
    "\n",
    "#Spacy load\n",
    "paradise_doc = nlp(paradise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')  Carroll\n",
       "4  ((, when, she, thought, it, over, afterwards, ...  Carroll"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract and store sentences from spacy doc\n",
    "paradise_sents = [[sent, 'Milton'] for sent in paradise_doc.sents]\n",
    "\n",
    "#Add the paradise sentences to our existing sentences DF\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents + paradise_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start getting inputs ready to create new dataframe, which will be fed into our supervised models.\n",
    "\n",
    "paradise_phrases = text_phrases(paradise_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_phrases = set(alice_phrases + persuasion_phrases + paradise_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the bag of words for paradise.\n",
    "paradisewords = bag_of_words(paradise_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords + paradisewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>squareness</th>\n",
       "      <th>the Musgroves</th>\n",
       "      <th>excited</th>\n",
       "      <th>a few hours</th>\n",
       "      <th>the dance</th>\n",
       "      <th>greatly</th>\n",
       "      <th>Art</th>\n",
       "      <th>property</th>\n",
       "      <th>pigs</th>\n",
       "      <th>...</th>\n",
       "      <th>bell</th>\n",
       "      <th>condemn</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>sent_punct_count</th>\n",
       "      <th>prev_sent_length</th>\n",
       "      <th>next_sent_length</th>\n",
       "      <th>num_words_repeated_from_prior_sent</th>\n",
       "      <th>upper_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>109</td>\n",
       "      <td>17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       squareness  the Musgroves excited  a few hours  the dance greatly  Art  \\\n",
       "0   65          0              0       0            0          0       0    0   \n",
       "1   61          0              0       0            0          0       0    0   \n",
       "2   29          0              0       0            0          0       0    0   \n",
       "3    9          0              0       0            0          0       0    0   \n",
       "4  122          0              0       0            0          0       0    0   \n",
       "\n",
       "  property  pigs     ...      bell  condemn  \\\n",
       "0        0     0     ...         0        0   \n",
       "1        0     0     ...         0        0   \n",
       "2        0     0     ...         0        0   \n",
       "3        0     0     ...         0        0   \n",
       "4        0     0     ...         0        0   \n",
       "\n",
       "                                       text_sentence text_source sent_length  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll          57   \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll          56   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll          29   \n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')     Carroll           6   \n",
       "4  ((, when, she, thought, it, over, afterwards, ...     Carroll         109   \n",
       "\n",
       "   sent_punct_count prev_sent_length next_sent_length  \\\n",
       "0                10              NaN             56.0   \n",
       "1                 7             57.0             29.0   \n",
       "2                 4             56.0              6.0   \n",
       "3                 3             29.0            109.0   \n",
       "4                17              6.0             21.0   \n",
       "\n",
       "  num_words_repeated_from_prior_sent  upper_case  \n",
       "0                                NaN           0  \n",
       "1                                0.0           0  \n",
       "2                                1.0           1  \n",
       "3                                2.0           0  \n",
       "4                                0.0           1  \n",
       "\n",
       "[5 rows x 5807 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  1 RFC Train Score: 0.994670456564\n",
      "Test:  1 RFC Test Score: 0.841320553781\n",
      "\n",
      "Test:  2 RFC Train Score: 0.993249244981\n",
      "Test:  2 RFC Test Score: 0.850905218317\n",
      "\n",
      "Test:  3 RFC Train Score: 0.99431616341\n",
      "Test:  3 RFC Test Score: 0.855010660981\n",
      "\n",
      "Test:  4 RFC Train Score: 0.99325044405\n",
      "Test:  4 RFC Test Score: 0.847547974414\n",
      "\n",
      "Test:  5 RFC Train Score: 0.994138543517\n",
      "Test:  5 RFC Test Score: 0.858208955224\n",
      "\n",
      "Test:  6 RFC Train Score: 0.992362344583\n",
      "Test:  6 RFC Test Score: 0.841151385928\n",
      "\n",
      "Test:  7 RFC Train Score: 0.992717584369\n",
      "Test:  7 RFC Test Score: 0.852878464819\n",
      "\n",
      "Average RFC Score: 0.84957474478\n"
     ]
    }
   ],
   "source": [
    "#Do not include SVC because it takes forever to run and it has poor performance\n",
    "X2 = word_counts.dropna()\n",
    "\n",
    "Y = np.array(X2['text_source'])\n",
    "X = np.array(X2.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "#Test accuracy using k-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lrscore = []\n",
    "rfcscore = []\n",
    "gbmscore = []\n",
    "svcscore = []\n",
    "\n",
    "testno = 1\n",
    "\n",
    "kf = KFold(n_splits=7, shuffle = True)\n",
    "for train_index, test_index in kf.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    train = rfc.fit(X_train, y_train)\n",
    "    print('Test: ', testno, 'RFC Train Score:', rfc.score(X_train, y_train))\n",
    "    print('Test: ', testno, 'RFC Test Score:', rfc.score(X_test, y_test))\n",
    "    rfcscore.append(rfc.score(X_test, y_test))\n",
    "    print('')\n",
    "    \n",
    "    testno += 1\n",
    "\n",
    "print('Average RFC Score:', np.mean(rfcscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27eceeb44e0>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEUCAYAAAAr20GQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27ece275d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the parts of speech distribution among each author\n",
    "viz_df = pd.DataFrame(word_counts.loc[:, parts])\n",
    "viz_df['Source'] = word_counts['text_source']\n",
    "carroll_df = viz_df[viz_df['Source'] == 'Carroll'].groupby('Source').sum() / (len(alice_doc))\n",
    "austen_df = viz_df[viz_df['Source'] == 'Austen'].groupby('Source').sum() / (len(persuasion_doc))\n",
    "milton_df = viz_df[viz_df['Source'] == 'Milton'].groupby('Source').sum() / (len(paradise_doc))\n",
    "new_df = carroll_df.append(austen_df).append(milton_df)\n",
    "\n",
    "new_df.T.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.841324200913\n"
     ]
    }
   ],
   "source": [
    "#Double-check the accuracy of the final model (testing between 3 authors)\n",
    "X2 = word_counts.dropna()\n",
    "\n",
    "Y = np.array(X2['text_source'])\n",
    "X = np.array(X2.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "from sklearn import ensemble \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.4, random_state = 0)\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
